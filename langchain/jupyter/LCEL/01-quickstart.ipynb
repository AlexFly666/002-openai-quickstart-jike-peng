{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexFly666/002-openai-quickstart-jike-peng/blob/main/langchain/jupyter/LCEL/01-quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03b42dcc-c644-45a6-8a6a-3b250c74cee7",
      "metadata": {
        "id": "03b42dcc-c644-45a6-8a6a-3b250c74cee7"
      },
      "source": [
        "## Langchain Expression Languageï¼ˆLCELï¼‰å¿«é€Ÿå…¥é—¨\n",
        "\n",
        "LCEL æ˜¯ LangChain ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œ**LCELæ˜¯ä¸€ç§å£°æ˜å¼çš„é“¾å¼ç»„åˆè¯­è¨€**ã€‚å®ƒæä¾›äº†ä¸€ç§ç»Ÿä¸€çš„æ¥å£ï¼Œå…è®¸ä¸åŒçš„ç»„ä»¶ï¼ˆå¦‚ `retriever`, `prompt`, `llm` ç­‰ï¼‰å¯ä»¥é€šè¿‡ç»Ÿä¸€çš„ `Runnable` æ¥å£è¿æ¥èµ·æ¥ã€‚æ¯ä¸ª `Runnable` ç»„ä»¶éƒ½å®ç°äº†ç›¸åŒçš„æ–¹æ³•ï¼Œå¦‚ `.invoke()`ã€`.stream()` æˆ– `.batch()`ï¼Œè¿™ä½¿å¾—å®ƒä»¬å¯ä»¥é€šè¿‡ `|` æ“ä½œç¬¦è½»æ¾è¿æ¥ã€‚\n",
        "\n",
        "### LCEL çš„ä¼˜åŠ¿\n",
        "\n",
        "LCELä½¿å¾—ä»åŸºæœ¬ç»„ä»¶æ„å»ºå¤æ‚é“¾å˜å¾—å®¹æ˜“ï¼Œå¹¶æ”¯æŒæµå¼å¤„ç†ã€å¹¶è¡Œå¤„ç†å’Œæ—¥å¿—è®°å½•ç­‰å¼€ç®±å³ç”¨çš„åŠŸèƒ½ã€‚\n",
        "\n",
        "- **ç»Ÿä¸€æ¥å£**: LCEL é€šè¿‡ `Runnable` æ¥å£å°†ä¸åŒçš„ç»„ä»¶ç»Ÿä¸€èµ·æ¥ï¼Œç®€åŒ–äº†å¤æ‚æ“ä½œçš„å®ç°ã€‚\n",
        "- **æ¨¡å—åŒ–**: å„ä¸ªç»„ä»¶å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•ï¼Œç„¶åé€šè¿‡ LCEL è½»æ¾é›†æˆã€‚\n",
        "- **å¯æ‰©å±•æ€§**: LCEL æ”¯æŒå¼‚æ­¥è°ƒç”¨ã€æ‰¹å¤„ç†å’Œæµå¼å¤„ç†ï¼Œé€‚åº”ä¸åŒçš„åº”ç”¨åœºæ™¯ã€‚\n",
        "\n",
        "\n",
        "### ç»„ä»¶\n",
        "\n",
        "æˆ‘ä»¬å·²å­¦ä¹ çš„ç»„ä»¶åŒ…æ‹¬ä»¥ä¸‹æ¨¡å—ï¼š\n",
        "\n",
        "#### ğŸ“ƒ Model I/Oï¼š\n",
        "\n",
        "è¿™åŒ…æ‹¬æç¤ºç®¡ç†ï¼Œæç¤ºä¼˜åŒ–ï¼Œç”¨äºèŠå¤©æ¨¡å‹å’ŒLLMçš„é€šç”¨æ¥å£ï¼Œä»¥åŠå¤„ç†æ¨¡å‹è¾“å‡ºçš„å¸¸è§å®ç”¨å·¥å…·ã€‚\n",
        "\n",
        "#### ğŸ“š Retrievalï¼š\n",
        "\n",
        "æ£€ç´¢å¢å¼ºç”Ÿæˆæ¶‰åŠä»å„ç§æ¥æºåŠ è½½æ•°æ®ï¼Œå‡†å¤‡æ•°æ®ï¼Œç„¶ååœ¨ç”Ÿæˆæ­¥éª¤ä¸­æ£€ç´¢æ•°æ®ä»¥ä¾›ä½¿ç”¨ã€‚\n",
        "\n",
        "#### ğŸ¤– Agentsï¼š\n",
        "\n",
        "Agents å…è®¸LLMè‡ªä¸»å®Œæˆä»»åŠ¡ã€‚ Agentsä¼šå†³å®šé‡‡å–å“ªäº›è¡ŒåŠ¨ï¼Œç„¶åæ‰§è¡Œè¯¥è¡ŒåŠ¨ï¼Œå¹¶è§‚å¯Ÿç»“æœï¼Œå¹¶é‡å¤æ­¤è¿‡ç¨‹ç›´åˆ°ä»»åŠ¡å®Œæˆã€‚ LangChainä¸ºä»£ç†æä¾›äº†æ ‡å‡†æ¥å£ã€å¯é€‰æ‹©çš„ä»£ç†åˆ—è¡¨ä»¥åŠç«¯åˆ°ç«¯ä»£ç†ç¤ºä¾‹ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b55489-48a5-4c2e-9a6e-3f246f0881c6",
      "metadata": {
        "id": "b2b55489-48a5-4c2e-9a6e-3f246f0881c6"
      },
      "source": [
        "### ä½¿ç”¨ LCEL å®ç° LLMChainï¼ˆPrompt + LLM)\n",
        "\n",
        "#### Pipe ç®¡é“æ“ä½œç¬¦\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨LCELçš„ `|` æ“ä½œç¬¦å°†è¿™äº›ä¸åŒç»„ä»¶æ‹¼æ¥æˆä¸€ä¸ªå•ä¸€é“¾ã€‚\n",
        "\n",
        "**åœ¨è¿™ä¸ªé“¾ä¸­ï¼Œç”¨æˆ·è¾“å…¥è¢«ä¼ é€’åˆ°æç¤ºæ¨¡æ¿ï¼Œç„¶åæç¤ºæ¨¡æ¿çš„è¾“å‡ºè¢«ä¼ é€’åˆ°æ¨¡å‹ï¼Œæ¥ç€æ¨¡å‹çš„è¾“å‡ºè¢«ä¼ é€’åˆ°è¾“å‡ºè§£æå™¨ã€‚**\n",
        "\n",
        "```python\n",
        "chain = prompt | model | output_parser\n",
        "```\n",
        "\n",
        "ç«–çº¿ç¬¦å·ç±»ä¼¼äºUnixç®¡é“æ“ä½œç¬¦ï¼Œå®ƒå°†ä¸åŒçš„ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œå°†ä¸€ä¸ªç»„ä»¶çš„è¾“å‡ºä½œä¸ºä¸‹ä¸€ä¸ªç»„ä»¶çš„è¾“å…¥ã€‚\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å½­è€å¸ˆçš„ç‰ˆæœ¬\n",
        "!pip install langchain==0.2.0\\\n",
        "langchain-openai==0.1.7\\\n",
        "langchain-core==0.2.1\\\n",
        "XXX\\\n",
        "langchain-experimental==0.0.59\\\n",
        "langchain-text-splitters==0.2.0\\\n",
        "langsmith==0.1.65\n",
        "\n",
        "# 202501æœ€æ–°ç‰ˆæœ¬\n",
        "# !pip install langchain\\\n",
        "# langchain-openai\\\n",
        "# langchain-core\\\n",
        "# langchain-community\\\n",
        "# langchain-experimental\\\n",
        "# langchain-text-splitters\\\n",
        "# langsmith\n",
        "\n",
        "# !pip install langchain==0.3.15 \\\n",
        "#     langchain-openai==0.3.2 \\\n",
        "#     langchain-core==0.3.31 \\\n",
        "#     langchain-community==0.3.15 \\\n",
        "#     langchain-experimental==0.3.4 \\\n",
        "#     langchain-text-splitters==0.3.5 \\\n",
        "#     langsmith==0.2.10"
      ],
      "metadata": {
        "collapsed": true,
        "id": "H5GNifRxAY07",
        "outputId": "7a3b0c7c-4e82-45bc-d60d-8726f1ff16a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "H5GNifRxAY07",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.2.0\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-openai==0.1.7\n",
            "  Downloading langchain_openai-0.1.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-core==0.2.1\n",
            "  Downloading langchain_core-0.2.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting langchain-community==0.2.0\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Collecting langchain-experimental==0.0.59\n",
            "  Downloading langchain_experimental-0.0.59-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langchain-text-splitters==0.2.0\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith==0.1.65\n",
            "  Downloading langsmith-0.1.65-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (3.11.11)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.2.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.7) (1.59.6)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai==0.1.7)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.1) (1.33)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core==0.2.1)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith==0.1.65) (3.10.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.0) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai==0.1.7) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai==0.1.7) (0.14.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'langsmith' candidate (version 0.1.65 at https://files.pythonhosted.org/packages/95/01/e66bf89a6be56236854cfd8f02abaf69a1bc13f31ba616dc517b89378d19/langsmith-0.1.65-py3-none-any.whl (from https://pypi.org/simple/langsmith/) (requires-python:<4.0,>=3.8.1))\n",
            "Reason for being yanked: When streaming llm tokens, since RunTree events default to null, LangChain core will raise an error\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.1.7-py3-none-any.whl (34 kB)\n",
            "Downloading langchain_core-0.2.1-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.0.59-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.5/199.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.65-py3-none-any.whl (124 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, packaging, mypy-extensions, typing-inspect, tiktoken, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community, langchain-experimental\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.2.10\n",
            "    Uninstalling langsmith-0.2.10:\n",
            "      Successfully uninstalled langsmith-0.2.10\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.29\n",
            "    Uninstalling langchain-core-0.3.29:\n",
            "      Successfully uninstalled langchain-core-0.3.29\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.5\n",
            "    Uninstalling langchain-text-splitters-0.3.5:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.5\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.14\n",
            "    Uninstalling langchain-0.3.14:\n",
            "      Successfully uninstalled langchain-0.3.14\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.0 langchain-community-0.2.0 langchain-core-0.2.1 langchain-experimental-0.0.59 langchain-openai-0.1.7 langchain-text-splitters-0.2.0 langsmith-0.1.65 marshmallow-3.26.0 mypy-extensions-1.0.0 packaging-23.2 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æŸ¥çœ‹ç‰ˆæœ¬\n",
        "!pip show langchain\\\n",
        "langchain-openai\\\n",
        "langchain-core\\\n",
        "langchain-community\\\n",
        "langchain-experimental\\\n",
        "langchain-text-splitters\\\n",
        "langsmith"
      ],
      "metadata": {
        "id": "SxTi34uVDGsQ",
        "outputId": "de23a95b-e305-4517-98c3-e45ec234b3b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "SxTi34uVDGsQ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.2.0\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-community\n",
            "---\n",
            "Name: langchain-openai\n",
            "Version: 0.1.7\n",
            "Summary: An integration package connecting OpenAI and LangChain\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core, openai, tiktoken\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-core\n",
            "Version: 0.2.1\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: jsonpatch, langsmith, packaging, pydantic, PyYAML, tenacity\n",
            "Required-by: langchain, langchain-community, langchain-experimental, langchain-openai, langchain-text-splitters\n",
            "---\n",
            "Name: langchain-community\n",
            "Version: 0.2.0\n",
            "Summary: Community contributed LangChain integrations.\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, dataclasses-json, langchain, langchain-core, langsmith, numpy, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: langchain-experimental\n",
            "---\n",
            "Name: langchain-experimental\n",
            "Version: 0.0.59\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-community, langchain-core\n",
            "Required-by: \n",
            "---\n",
            "Name: langchain-text-splitters\n",
            "Version: 0.2.0\n",
            "Summary: LangChain text splitting utilities\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: langchain-core\n",
            "Required-by: langchain\n",
            "---\n",
            "Name: langsmith\n",
            "Version: 0.1.65\n",
            "Summary: Client library to connect to the LangSmith LLM Tracing and Evaluation Platform.\n",
            "Home-page: https://smith.langchain.com/\n",
            "Author: LangChain\n",
            "Author-email: support@langchain.dev\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: orjson, pydantic, requests\n",
            "Required-by: langchain, langchain-community, langchain-core\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "09242ff5-b5ff-4902-9de8-59e4af945bf2",
      "metadata": {
        "id": "09242ff5-b5ff-4902-9de8-59e4af945bf2",
        "outputId": "008e3fd6-0341-4725-fbc6-db03a03617d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'æ˜¥è¿æœŸé—´ï¼Œå°æ˜å†³å®šåç«è½¦å›å®¶ã€‚ä»–åœ¨ç«è½¦ç«™é‡åˆ°äº†ä¸€ä¸ªå–èŒ¶å¶è›‹çš„å¤§å¦ˆã€‚\\n\\nå°æ˜ï¼šå¤§å¦ˆï¼Œä½ è¿™èŒ¶å¶è›‹å¤šå°‘é’±ä¸€ä¸ªï¼Ÿ\\n\\nå¤§å¦ˆï¼š5å—é’±3ä¸ªã€‚\\n\\nå°æ˜å¿ƒæƒ³ï¼šè¿™ä»·æ ¼æœ‰ç‚¹è´µå•Šï¼Œä½†è¿˜æ˜¯ä¹°äº†3ä¸ªå°å°é²œã€‚\\n\\nåƒå®ŒèŒ¶å¶è›‹åï¼Œå°æ˜è§‰å¾—å‘³é“è¿˜ä¸é”™ï¼Œäºæ˜¯åˆå»é—®å¤§å¦ˆï¼š\\n\\nå°æ˜ï¼šå¤§å¦ˆï¼Œä½ è¿™èŒ¶å¶è›‹çœŸçš„å¾ˆå¥½åƒï¼Œèƒ½ä¸èƒ½ä¾¿å®œç‚¹å‘€ï¼Ÿ\\n\\nå¤§å¦ˆï¼šå°ä¼™å­ï¼Œçœ‹ä½ è¿™ä¹ˆå–œæ¬¢ï¼Œæˆ‘å†ç»™ä½ ä¼˜æƒ ä¸€ä¸‹ï¼Œ4å—é’±2ä¸ªå§ï¼\\n\\nå°æ˜é«˜å…´åœ°ä¹°ä¸‹äº†2ä¸ªèŒ¶å¶è›‹ï¼Œè¾¹èµ°è¾¹å›å‘³ç€åˆšæ‰çš„ç¾å‘³ã€‚\\n\\nçªç„¶ï¼Œå°æ˜æƒ³èµ·ä¸€ä»¶äº‹ï¼Œèµ¶ç´§è·‘å›å»æ‰¾å¤§å¦ˆï¼š\\n\\nå°æ˜ï¼šå¤§å¦ˆï¼Œæˆ‘åˆšåˆšå¿˜äº†é—®ä½ ï¼Œä½ è¿™èŒ¶å¶è›‹æ˜¯æ€ä¹ˆåšçš„ï¼Ÿæˆ‘ä¹Ÿæƒ³å­¦å­¦ï¼\\n\\nå¤§å¦ˆä¸€è„¸ç–‘æƒ‘åœ°çœ‹ç€å°æ˜ï¼šå°ä¼™å­ï¼Œæˆ‘è¿™èŒ¶å¶è›‹å°±æ˜¯ç”¨æ°´ç…®çš„å‘€ï¼Œæœ‰ä»€ä¹ˆç‰¹åˆ«çš„å—ï¼Ÿ\\n\\nå°æ˜æç„¶å¤§æ‚Ÿï¼ŒåŸæ¥è‡ªå·±åƒçš„â€œèŒ¶å¶è›‹â€å…¶å®å°±æ˜¯æ™®é€šçš„é¸¡è›‹ï¼ä¸ç¦æ„Ÿå¹é“ï¼š\\n\\nå°æ˜ï¼šå”‰ï¼Œçœ‹æ¥æˆ‘è¿˜æ˜¯å¤ªå¤©çœŸäº†ï¼Œæ˜¥è¿å›å®¶çš„è·¯ä¸Šä¸ä»…è¦æŒ¤ç«è½¦ï¼Œè¿˜è¦å°å¿ƒè¢«å‘å•Šï¼'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# OpenAI å®¢æˆ·ç«¯è°ƒç”¨\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# åˆå§‹åŒ– ChatOpenAI æ¨¡å‹ï¼ŒæŒ‡å®šä½¿ç”¨çš„æ¨¡å‹ä¸º 'gpt-4o-mini'\n",
        "# model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "\n",
        "# ZhipuAI å®¢æˆ·ç«¯è°ƒç”¨\n",
        "# å‚è€ƒï¼šhttps://python.langchain.com/v0.2/api_reference/community/chat_models/langchain_community.chat_models.zhipuai.ChatZhipuAI.html\n",
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "model = ChatZhipuAI(\n",
        "    temperature=0.95,\n",
        "    model=\"glm-4v-flash\",\n",
        "    api_key=\"v\"\n",
        "    # api_base=\"...\",\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿ï¼Œè®¾ç½®æ¨¡æ¿å†…å®¹ä¸º\"è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§\"\n",
        "prompt = ChatPromptTemplate.from_template(\"è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§\")\n",
        "\n",
        "# åˆå§‹åŒ–è¾“å‡ºè§£æå™¨ï¼Œç”¨äºå°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# æ„å»ºä¸€ä¸ªå¤„ç†é“¾ï¼Œå…ˆé€šè¿‡æç¤ºæ¨¡æ¿ç”Ÿæˆå®Œæ•´çš„è¾“å…¥ï¼Œç„¶åé€šè¿‡æ¨¡å‹å¤„ç†ï¼Œæœ€åè§£æè¾“å‡º\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "# è°ƒç”¨å¤„ç†é“¾ï¼Œä¼ å…¥ä¸»é¢˜ä¸º\"ç¨‹åºå‘˜\"ï¼Œç”Ÿæˆå…³äºç¨‹åºå‘˜çš„ç¬‘è¯\n",
        "chain.invoke({\"topic\": \"ç¨‹åºå‘˜\"})\n",
        "\n",
        "chain.invoke({\"topic\": \"æ˜¥è¿\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9f4cd1e-7d55-4b14-ae37-9f7d66b07892",
      "metadata": {
        "id": "a9f4cd1e-7d55-4b14-ae37-9f7d66b07892"
      },
      "source": [
        "## æ ¸å¿ƒæ¦‚å¿µï¼šinvoke æ–¹æ³•\n",
        "\n",
        "Langchain `invoke` æ–¹æ³•æ˜¯ LCEL è®¾è®¡ä¸­çš„é‡è¦æ–¹æ³•ï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…æ›´é«˜æ•ˆåœ°å¤„ç†å¤æ‚ä»»åŠ¡ï¼Œç»“åˆè¯­è¨€æ¨¡å‹è¿›è¡Œç³»ç»Ÿæ„å»ºï¼Œå®ç°ä¸åŒæ•°æ®æºå’ŒAPIçš„æ¥å£å¯¹æ¥ã€‚\n",
        "\n",
        "### åŸºç¡€æ¦‚å¿µ\n",
        "\n",
        "- åœ¨Langchainä¸­ï¼Œinvokeæ–¹æ³•æ˜¯æ‰€æœ‰LangChainè¡¨è¾¾å¼è¯­è¨€ï¼ˆLCELï¼‰å¯¹è±¡çš„é€šç”¨åŒæ­¥è°ƒç”¨æ–¹æ³•ã€‚é€šè¿‡invokeæ–¹æ³•ï¼Œå¼€å‘è€…å¯ä»¥ç›´æ¥è°ƒç”¨LLMæˆ–ChatModelï¼Œç®€åŒ–äº†è°ƒç”¨æµç¨‹ï¼Œæé«˜äº†å¼€å‘æ•ˆç‡ã€‚\n",
        "- ä¸å…¶ä»–é“¾å¼è°ƒç”¨æ–¹æ³•ç›¸æ¯”ï¼Œinvokeæ–¹æ³•æ›´åŠ çµæ´»ä¾¿æ·ï¼Œå¯ä»¥ç›´æ¥å¯¹è¾“å…¥è¿›è¡Œè°ƒç”¨ï¼Œè€Œä¸éœ€è¦é¢å¤–çš„é“¾å¼æ“ä½œã€‚ç›¸å¯¹äºbatchå’Œstreamç­‰å¼‚æ­¥æ–¹æ³•ï¼Œinvokeæ–¹æ³•æ›´é€‚ç”¨äºå•ä¸€æ“ä½œçš„æ‰§è¡Œã€‚\n",
        "\n",
        "### ä½¿ç”¨æ–¹å¼\n",
        "\n",
        "- å•æ¬¡è°ƒç”¨ï¼šé€šè¿‡invokeæ–¹æ³•ï¼Œå¼€å‘è€…å¯ä»¥å¿«é€Ÿå¯¹å•ä¸€æ“ä½œè¿›è¡Œæ‰§è¡Œï¼Œä¾‹å¦‚è½¬æ¢ChatMessageä¸ºPythonå­—ç¬¦ä¸²ç­‰ç®€å•æ“ä½œï¼Œæå‡äº†ä»£ç çš„å¯è¯»æ€§å’Œæ•´æ´åº¦ã€‚\n",
        "- å¤æ‚åä½œï¼šLangchainçš„æ ¸å¿ƒç†å¿µå°±æ˜¯å°†è¯­è¨€æ¨¡å‹ä½œä¸ºåä½œå·¥å…·ï¼Œinvokeæ–¹æ³•å¯ä»¥å¾ˆå¥½åœ°å®ç°å¼€å‘è€…ä¸è¯­è¨€æ¨¡å‹çš„é«˜æ•ˆäº’åŠ¨ï¼Œæ„å»ºå‡ºå¤„ç†å¤æ‚ä»»åŠ¡çš„ç³»ç»Ÿï¼Œå¹¶å¯¹æ¥ä¸åŒçš„æ•°æ®æºå’ŒAPIæ¥å£ã€‚\n",
        "\n",
        "## invoke ä¸ Model I/O çš„ç»“åˆ\n",
        "\n",
        "æ•´ä¸ªæµç¨‹æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š\n",
        "\n",
        "1. `Prompt` ç»„ä»¶æ¥æ”¶ç”¨æˆ·è¾“å…¥ **{\"topic\": \"ç¨‹åºå‘˜\"}**ï¼Œç„¶åä½¿ç”¨è¯¥ topic æ„å»º `PromptValue`\n",
        "1. `Model` ç»„ä»¶è·å–ç”Ÿæˆçš„æç¤ºï¼Œå¹¶ä¼ é€’ç»™ GPT-3.5-Turbo æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚ä»æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªChatMessageå¯¹è±¡ã€‚\n",
        "1. æœ€åï¼Œ`output_parser` ç»„ä»¶æ¥æ”¶ChatMessageï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºPythonå­—ç¬¦ä¸²ï¼Œåœ¨invokeæ–¹æ³•ä¸­è¿”å›ã€‚\n",
        "\n",
        "### Prompt\n",
        "\n",
        "`prompt` æ˜¯ `BasePromptTemplate` çš„å®ä¾‹ï¼Œè¿™æ„å‘³ç€å®ƒæ¥å—æ¨¡æ¿å˜é‡çš„å­—å…¸å¹¶ç”Ÿæˆä¸€ä¸ª`PromptValue`ã€‚\n",
        "\n",
        "PromptValueæ˜¯ä¸€ä¸ªåŒ…è£…å™¨(wrapper)ï¼Œå›´ç»•å®Œæˆçš„æç¤ºè¿›è¡Œæ“ä½œï¼Œå¯ä»¥ä¼ é€’ç»™LLMï¼ˆä»¥å­—ç¬¦ä¸²ä½œä¸ºè¾“å…¥ï¼‰æˆ–ChatModelï¼ˆä»¥æ¶ˆæ¯åºåˆ—ä½œä¸ºè¾“å…¥ï¼‰ã€‚\n",
        "\n",
        "å®ƒå¯ä»¥ä¸ä»»ä½•è¯­è¨€æ¨¡å‹ç±»å‹ä¸€èµ·ä½¿ç”¨ï¼Œå› ä¸ºå®ƒå®šä¹‰äº†ç”Ÿæˆ`BaseMessages`å’Œç”Ÿæˆå­—ç¬¦ä¸²çš„é€»è¾‘ã€‚\n",
        "\n",
        "```python\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "# Prompt é LCEL ä½¿ç”¨æ–¹æ³•\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"è®²ä¸ªå…³äº {topic} çš„ç¬‘è¯å§\"\n",
        ")\n",
        "\n",
        "# ä½¿ç”¨ format ç”Ÿæˆæç¤º\n",
        "prompt = prompt_template.format(topic=\"ç¨‹åºå‘˜\")\n",
        "print(prompt)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0d4b0ead-76d5-43e1-ba40-590b20894c43",
      "metadata": {
        "id": "0d4b0ead-76d5-43e1-ba40-590b20894c43",
        "outputId": "7d91eba1-205d-43cf-f9bc-00a57603a796",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='è®²ä¸ªå…³äº ç¨‹åºå‘˜ çš„ç¬‘è¯å§')])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# è°ƒç”¨ Prompt çš„ invoke æ–¹æ³•ç”Ÿæˆæœ€ç»ˆçš„æç¤ºè¯\n",
        "prompt_value = prompt.invoke({\"topic\": \"ç¨‹åºå‘˜\"})\n",
        "prompt_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ea46f7e-67cb-44c5-a5c1-6311a2a75639",
      "metadata": {
        "id": "7ea46f7e-67cb-44c5-a5c1-6311a2a75639",
        "outputId": "b30ce6ab-1e76-4a07-da63-f08d3b3247e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='è®²ä¸ªå…³äº ç¨‹åºå‘˜ çš„ç¬‘è¯å§')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# é€‚ç”¨äº ChatModel çš„ Message æ ¼å¼\n",
        "prompt_value.to_messages()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7b6030b5-f19f-42ee-9405-0bc229bd3c7b",
      "metadata": {
        "id": "7b6030b5-f19f-42ee-9405-0bc229bd3c7b",
        "outputId": "532209f8-04f7-4da2-92a4-1b0b93cde23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: è®²ä¸ªå…³äº ç¨‹åºå‘˜ çš„ç¬‘è¯å§'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# å­—ç¬¦ä¸²æ ¼å¼\n",
        "prompt_value.to_string()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "781d7c57-fd05-44b2-9638-2923ff532e0e",
      "metadata": {
        "id": "781d7c57-fd05-44b2-9638-2923ff532e0e"
      },
      "source": [
        "### Model\n",
        "\n",
        "ç„¶åè°ƒç”¨æ¨¡å‹çš„ `invoke` æ–¹æ³•ï¼Œå°† `PromptValue` ä¼ é€’ç»™æ¨¡å‹ã€‚\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨çš„ `GPT-4o-mini` æ¨¡å‹æ˜¯ ChatModelï¼Œinvoke æ–¹æ³•å°†è¿”å› BaseMessageã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c82e786d-52f1-4082-92b5-00f3029d5983",
      "metadata": {
        "id": "c82e786d-52f1-4082-92b5-00f3029d5983"
      },
      "outputs": [],
      "source": [
        "message = model.invoke(prompt_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2a6a7a85-c1da-4441-a1db-cab6d0bd32a0",
      "metadata": {
        "id": "2a6a7a85-c1da-4441-a1db-cab6d0bd32a0",
        "outputId": "29190b23-d429-402c-973c-edc63b695293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='ä¸ºä»€ä¹ˆç¨‹åºå‘˜æ€»æ˜¯æºå¸¦è®¡ç®—å™¨ï¼Ÿ\\nå› ä¸ºä»–ä»¬ä¸æƒ³è¢«äººç§°ä¸ºâ€œç –å®¶â€ã€‚', response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 70, 'total_tokens': 86}, 'model_name': 'glm-4v-flash', 'finish_reason': 'stop'}, id='run-425fd3ac-0e1b-44b2-9dda-dacfc3817c4f-0')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e36bb7-c7b3-49f1-9e6e-3a04c2d39d46",
      "metadata": {
        "id": "f4e36bb7-c7b3-49f1-9e6e-3a04c2d39d46"
      },
      "source": [
        "å¦‚æœæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ LLM æ¨¡å‹  `gpt-3.5-turbo-instruct`ï¼Œinvoke æ–¹æ³•å°±ä¼šè¿”å›å­—ç¬¦ä¸²ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7b56176b-65d6-41f4-8071-058593e47a2b",
      "metadata": {
        "id": "7b56176b-65d6-41f4-8071-058593e47a2b",
        "outputId": "a8a36a87-73d3-417b-a2dd-fe25915086ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for Generation\ntext\n  none is not an allowed value (type=type_error.none.not_allowed)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f5334b8f427b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         return (\n\u001b[0;32m--> 276\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    632\u001b[0m         \u001b[0mprompt_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m                 )\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[0;32m--> 803\u001b[0;31m             output = self._generate_helper(\n\u001b[0m\u001b[1;32m    804\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m         \u001b[0mflattened_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/llms.py\u001b[0m in \u001b[0;36m_generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             output = (\n\u001b[0;32m--> 657\u001b[0;31m                 self._generate(\n\u001b[0m\u001b[1;32m    658\u001b[0m                     \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/llms/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msystem_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0msystem_fingerprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"system_fingerprint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         return self.create_llm_result(\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/llms/base.py\u001b[0m in \u001b[0;36mcreate_llm_result\u001b[0;34m(self, choices, prompts, params, token_usage, system_fingerprint)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0msub_choices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             generations.append(\n\u001b[0;32m--> 473\u001b[0;31m                 [\n\u001b[0m\u001b[1;32m    474\u001b[0m                     Generation(\n\u001b[1;32m    475\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/llms/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    472\u001b[0m             generations.append(\n\u001b[1;32m    473\u001b[0m                 [\n\u001b[0;32m--> 474\u001b[0;31m                     Generation(\n\u001b[0m\u001b[1;32m    475\u001b[0m                         \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                         generation_info=dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/v1/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalidation_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mobject_setattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__dict__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Generation\ntext\n  none is not an allowed value (type=type_error.none.not_allowed)"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAI\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "\n",
        "# # OpenAI APIè°ƒç”¨ï¼ˆä»£ç†æ–¹å¼ï¼‰\n",
        "llm = OpenAI(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    api_key=\"XXX\",\n",
        "    base_url=\"https://vip.apiyi.com/v1\"\n",
        ")\n",
        "\n",
        "\n",
        "llm.invoke(prompt_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de91e1e1-c670-4803-a7d6-6b05df266961",
      "metadata": {
        "id": "de91e1e1-c670-4803-a7d6-6b05df266961"
      },
      "source": [
        "### Output Parser\n",
        "\n",
        "æœ€åï¼Œæˆ‘ä»¬å°†æ¨¡å‹è¾“å‡ºä¼ é€’ç»™ output_parserï¼Œå®ƒæ˜¯ä¸€ä¸ª `BaseOutputParser` ç¤ºä¾‹ï¼Œæ„å‘³ç€å®ƒæ¥å—å­—ç¬¦ä¸²æˆ– BaseMessage ä½œä¸ºè¾“å…¥ã€‚\n",
        "\n",
        "æœ¬æŒ‡å—ä¸­ä½¿ç”¨çš„ `StrOutputParser` ç¤ºä¾‹å°†æ‰€æœ‰è¾“å…¥éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "150ceaf0-fcc1-4756-8fca-ab9bf28ed94a",
      "metadata": {
        "id": "150ceaf0-fcc1-4756-8fca-ab9bf28ed94a"
      },
      "outputs": [],
      "source": [
        "# message ç»è¿‡ StrOutputParser å¤„ç†ï¼Œå˜ä¸ºæ ‡å‡†çš„å­—ç¬¦ä¸²\n",
        "output_parser.invoke(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c5e1741-83a4-4312-be93-76523e2c9373",
      "metadata": {
        "id": "2c5e1741-83a4-4312-be93-76523e2c9373"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c02797bf-df69-4c7d-ba0a-35df38d53173",
      "metadata": {
        "id": "c02797bf-df69-4c7d-ba0a-35df38d53173"
      },
      "source": [
        "## Invoke ä¸ Retrieval ç»“åˆ\n",
        "\n",
        "ä¸‹é¢æ¼”ç¤ºå¦‚ä½•åœ¨ç»å…¸çš„ RAG åœºæ™¯ä¸­ä½¿ç”¨ invoke æ–¹æ³•ã€‚ä¸‹é¢å°†ä½¿ç”¨`|`æ“ä½œç¬¦å®ç°æ›´å¤æ‚çš„é“¾å¼è°ƒç”¨ã€‚\n",
        "\n",
        "```python\n",
        "chain = setup_and_retrieval | prompt | model | output_parser\n",
        "```\n",
        "ä¸ºäº†è§£é‡Šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆå¯ä»¥çœ‹åˆ°ä¸Šé¢çš„æç¤ºæ¨¡æ¿æ¥å—ä¸Šä¸‹æ–‡å’Œé—®é¢˜ä½œä¸ºè¦æ›¿æ¢åœ¨æç¤ºä¸­çš„å€¼ã€‚åœ¨æ„å»ºæç¤ºæ¨¡æ¿ä¹‹å‰ï¼Œæˆ‘ä»¬å¸Œæœ›æ£€ç´¢ç›¸å…³æ–‡ä»¶ä»¥åŠå°†å®ƒä»¬åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
        "\n",
        "ä½œä¸ºåˆæ­¥æ­¥éª¤ï¼Œæˆ‘ä»¬å·²ç»è®¾ç½®äº†ä½¿ç”¨å†…å­˜å­˜å‚¨å™¨çš„æ£€ç´¢å™¨ï¼Œå®ƒå¯ä»¥æ ¹æ®æŸ¥è¯¢æ£€ç´¢æ–‡æ¡£ã€‚è¿™ä¹Ÿæ˜¯ä¸€ä¸ªå¯è¿è¡Œçš„ç»„ä»¶ï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–ç»„ä»¶é“¾æ¥åœ¨ä¸€èµ·ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥å°è¯•å•ç‹¬è¿è¡Œå®ƒï¼š\n",
        "\n",
        "æ•´ä¸ªæµç¨‹å¦‚ä¸‹ï¼š\n",
        "\n",
        "1. é¦–å…ˆåˆ›å»ºä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ¡ç›®(entries)çš„ `RunnableParallel` å¯¹è±¡ **setup_and_retrieval**ã€‚ç¬¬ä¸€ä¸ªæ¡ç›®`context`å°†åŒ…æ‹¬æ£€ç´¢å™¨è·å–çš„æ–‡æ¡£ç»“æœã€‚ç¬¬äºŒä¸ªæ¡ç›®`question`å°†åŒ…å«ç”¨æˆ·åŸå§‹é—®é¢˜ã€‚ä¸ºäº†ä¼ é€’é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨`RunnablePassthrough`æ¥å¤åˆ¶è¿™ä¸ªæ¡ç›®ã€‚\n",
        "2. å°†ä¸Šä¸€æ­¥ä¸­çš„å­—å…¸æä¾›ç»™`Prompt`ç»„ä»¶ã€‚ç„¶åï¼Œå®ƒæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼ˆå³é—®é¢˜ï¼‰ä»¥åŠæ£€ç´¢åˆ°çš„æ–‡æ¡£ï¼ˆå³contextï¼‰ï¼Œæ„å»ºæç¤ºå¹¶è¾“å‡º`PromptValue`ã€‚\n",
        "3. `Model` ç»„ä»¶æ¥å—ç”Ÿæˆçš„æç¤ºï¼Œå¹¶ä¼ é€’ç»™OpenAI `gpt-4o-mini` æ¨¡å‹è¿›è¡Œè¯„ä¼°ã€‚æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºæ˜¯ä¸€ä¸ªChatMessageå¯¹è±¡ã€‚\n",
        "4. æœ€åï¼Œ`output_parser` ç»„ä»¶æ¥æ”¶ChatMessageï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºPythonå­—ç¬¦ä¸²ï¼Œåœ¨è°ƒç”¨æ–¹æ³•ä¸­è¿”å›è¯¥å­—ç¬¦ä¸²ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa1a415-c38e-44c3-bda1-7e50aa6f7b46",
      "metadata": {
        "id": "6aa1a415-c38e-44c3-bda1-7e50aa6f7b46"
      },
      "outputs": [],
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ZhipuAI å®¢æˆ·ç«¯è°ƒç”¨\n",
        "# å‚è€ƒï¼šhttps://python.langchain.com/v0.2/api_reference/community/chat_models/langchain_community.chat_models.zhipuai.ChatZhipuAI.html\n",
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "model = ChatZhipuAI(\n",
        "    temperature=0.95,\n",
        "    model=\"glm-4v-flash\",\n",
        "    api_key=\"7863f6f8faac00a75e3bae5a78235a3c.8d4pbUUYUrbo92yY\"\n",
        "    # api_base=\"...\",\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "4L2eSlbUOepd"
      },
      "id": "4L2eSlbUOepd",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzZD7V7jPBQ0"
      },
      "id": "hzZD7V7jPBQ0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "188130e3-54d8-4efe-a40e-c0ab81f9e274",
      "metadata": {
        "id": "188130e3-54d8-4efe-a40e-c0ab81f9e274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "5ebd0987-1a94-4144-a2e7-cd1a5e62531e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'ZhipuAIEmbeddings' from 'langchain_community.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-abd9e38a5efe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunnables\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunnableParallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnablePassthrough\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from langchain_openai import OpenAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZhipuAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# ä½¿ç”¨ DocArrayInMemorySearch åˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„å‘é‡å­˜å‚¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'ZhipuAIEmbeddings' from 'langchain_community.embeddings' (/usr/local/lib/python3.11/dist-packages/langchain_community/embeddings/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# å¯¼å…¥ LangChain åº“çš„ä¸åŒæ¨¡å—ï¼ŒåŒ…æ‹¬å‘é‡å­˜å‚¨ã€è¾“å‡ºè§£æå™¨ã€æç¤ºæ¨¡æ¿ã€å¹¶è¡Œè¿è¡Œå™¨å’Œ OpenAI çš„åµŒå…¥æ¨¡å‹\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
        "\n",
        "# ä½¿ç”¨ DocArrayInMemorySearch åˆ›å»ºä¸€ä¸ªå†…å­˜ä¸­çš„å‘é‡å­˜å‚¨\n",
        "# ä½¿ç”¨ OpenAIEmbeddings ä¸ºæ–‡æœ¬ç”ŸæˆåµŒå…¥å‘é‡ï¼Œæ–‡æœ¬ä¸º \"harrison worked at kensho\" å’Œ \"bears like to eat honey\"\n",
        "vectorstore = DocArrayInMemorySearch.from_texts(\n",
        "    [\"harrison worked at kensho\", \"bears like to eat honey\"],\n",
        "    embedding=ZhipuAIEmbeddings(),\n",
        ")\n",
        "\n",
        "# å°†å‘é‡å­˜å‚¨è½¬æ¢ä¸ºæ£€ç´¢å™¨\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# åˆ›å»ºä¸€ä¸ªèŠå¤©æç¤ºæ¨¡æ¿ï¼Œç”¨ä¸­æ–‡è®¾ç½®æ¨¡æ¿ä»¥ä¾¿ç”ŸæˆåŸºäºç‰¹å®šä¸Šä¸‹æ–‡å’Œé—®é¢˜çš„å®Œæ•´è¾“å…¥\n",
        "template = \"\"\"æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜:\n",
        "{context}\n",
        "\n",
        "é—®é¢˜: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# åˆå§‹åŒ–è¾“å‡ºè§£æå™¨ï¼Œå°†æ¨¡å‹è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# è®¾ç½®ä¸€ä¸ªå¹¶è¡Œè¿è¡Œå™¨ï¼Œç”¨äºåŒæ—¶å¤„ç†ä¸Šä¸‹æ–‡æ£€ç´¢å’Œé—®é¢˜ä¼ é€’\n",
        "# ä½¿ç”¨RunnableParallelæ¥å‡†å¤‡é¢„æœŸçš„è¾“å…¥ï¼Œé€šè¿‡ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£æ¡ç›®ä»¥åŠåŸå§‹ç”¨æˆ·é—®é¢˜ï¼Œ\n",
        "# åˆ©ç”¨æ–‡æ¡£æœç´¢å™¨ retriever è¿›è¡Œæ–‡æ¡£æœç´¢ï¼Œå¹¶ä½¿ç”¨ RunnablePassthrough æ¥ä¼ é€’ç”¨æˆ·çš„é—®é¢˜ã€‚\n",
        "setup_and_retrieval = RunnableParallel(\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "# æ„å»ºä¸€ä¸ªå¤„ç†é“¾ï¼ŒåŒ…æ‹¬ä¸Šä¸‹æ–‡å’Œé—®é¢˜çš„è®¾ç½®ã€æç¤ºç”Ÿæˆã€æ¨¡å‹è°ƒç”¨å’Œè¾“å‡ºè§£æ\n",
        "chain = setup_and_retrieval | prompt | model | output_parser\n",
        "\n",
        "# è°ƒç”¨å¤„ç†é“¾ï¼Œä¼ å…¥é—®é¢˜\"where did harrison work?\"ï¼ˆéœ€ç¿»è¯‘ä¸ºä¸­æ–‡ï¼‰ï¼Œå¹¶åŸºäºç»™å®šçš„æ–‡æœ¬ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ\n",
        "chain.invoke(\"harrisonåœ¨å“ªé‡Œå·¥ä½œï¼Ÿ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bcecfd-e726-4570-bea6-19cd5b8fbd15",
      "metadata": {
        "id": "93bcecfd-e726-4570-bea6-19cd5b8fbd15"
      },
      "source": [
        "#### å¿½ç•¥è­¦å‘Šæç¤ºï¼š\n",
        "\n",
        "UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
        "\n",
        "åŸå› ï¼š`The issue is with pydantic version, it's 2.0.0+ and not compatible with docarray.\n",
        "Instead it should be pydantic==1.10.9`\n",
        "\n",
        "å‚è€ƒï¼šhttps://github.com/langchain-ai/langchain/issues/15394\n",
        "\n",
        "LangChainå®˜æ–¹å…³äº Pydatic å…¼å®¹æ€§çš„è¯´æ˜ï¼šhttps://python.langchain.com/v0.1/docs/guides/development/pydantic_compatibility/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7027c6-d85a-4af7-b5ef-1949d88f082f",
      "metadata": {
        "id": "cb7027c6-d85a-4af7-b5ef-1949d88f082f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "800ffa20-782d-4f83-85e7-4349f8121186",
      "metadata": {
        "id": "800ffa20-782d-4f83-85e7-4349f8121186"
      },
      "source": [
        "### Homework: ä½¿ç”¨æŒä¹…åŒ–å­˜å‚¨çš„å‘é‡æ•°æ®åº“æ›¿æ¢ DocArrayInMemorySearchï¼Œé‡å†™ LCEL ç‰ˆæœ¬çš„ RAG ç¤ºä¾‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82210b4c-df52-4b7f-abf2-97211ba9a7ec",
      "metadata": {
        "id": "82210b4c-df52-4b7f-abf2-97211ba9a7ec"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}